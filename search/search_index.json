{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Scientist? The evolution of people having the ability to intepret data has evolved over the years. Statistician, Data Analyst, Data Scientist, and now a new kid of the block, AI scientist is borne. They are more or less related in various ways with varying interpretations, but to me, the latter is predominantly involved in predictive learning solutions using Neural Networks (NN). This page will deal with the basics of NN, including the various frameworks, transfer-learning, and other tools that will allow one to kick start his/her career. Don't worry, it is not as hard once you get a grasp of it! It's not that hard, once you get the hang of it! Do check out my accompanying website on AI Engineering that provides essential guides on engineering aspects of machine learning.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-scientist","text":"The evolution of people having the ability to intepret data has evolved over the years. Statistician, Data Analyst, Data Scientist, and now a new kid of the block, AI scientist is borne. They are more or less related in various ways with varying interpretations, but to me, the latter is predominantly involved in predictive learning solutions using Neural Networks (NN). This page will deal with the basics of NN, including the various frameworks, transfer-learning, and other tools that will allow one to kick start his/her career. Don't worry, it is not as hard once you get a grasp of it! It's not that hard, once you get the hang of it! Do check out my accompanying website on AI Engineering that provides essential guides on engineering aspects of machine learning.","title":"So You Wana Be an AI Scientist?"},{"location":"ann/","text":"Basics of Neural Network A perceptron is the simplest unit of a neural network. The input neurons first weights its inputs and then sums them up with a constant, called bias . An activation function is then applied, which produces the output for the neuron. x1*w1 + x2*w2 + x3*w3 + bias = y A deep neural network simply represents a neural network with many hidden layers between the input and output layers. The architecture of the hidden layers can be very complex, like CNN or LSTM. Activation Function An activation function tells a perception what outcome it should be. The function differs for input/hidden layers with output layers. For input/hidden layers, ReLu (Rectified Linear units) is very popular compared to the now mostly obsolete sigmoid & tanh functions because it avoids the vanishing gradient problem and has faster convergence. However, it is susceptible to dead neurons. So variants like Leaky ReLu, MaxOut and other functions are created to address this. For the output layer, it depends on the type of learning we are trying to train. Output Type Function Binary Classification Sigmoid Multi-Class Classification Softmax Regression Linear Backpropagation Backpropagation is a short form for \"backward propagation of errors.\" Training a model is about minimising the loss, and to do that, fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration. This is done by working backwards from the last layer to the first input layer. There are various essential terms that needs to be defined in an NN during training. Optimizer Optimizers are learning algothrims used to change the weights & biases of the neural network optimially to obtain the minimal loss. Gradient descent is the most classic of them, while the most widely used optimizer now is Adam (Adaptive Moment Estimation). More in this article . Optimizer Desc Gradient Descent Most basic & classic Adam Most popular & the current best. Adaptive learning using EWMA on 1st & 2nd moments Rmsprop 2nd most popular. EWMA on squared gradient adagrad Adagrad Able to train for sparse data. Adaptive learning using squared gradient Loss Function The loss (or cost) function which we define as the approximation for data loss. It is thus important to choose one that best represents the type of data and learning. Below are some examples. Type Loss Function Binary Classification binary_crossentropy Multi-class Classification categorical_crossentropy Regression mse Learning Rate Learning Rate (lr), or step size is the most important parameter to adjust when using an optimizer. Too large a lr can cause the model to be unable to find the minimal loss, whereas a lr that is too small can cause the training process to take too long. Using a large vs small learning rate to reach minimum loss To check if you are using a good learning rate, we can plot the loss over epoch. It should ideally drop gradually to a consistent rate over time. Loss over epoch for varying learning rates. Source Batch Size & Epoch Because of memory limitations, we cannot feed the entire training data to the network at one go, but divide them into batches. Given a total training sample size of 400, and a batch size of 4, we will require 400/4 = 100 iterations to complete one epoch which is the completion of training all sample sizes in a loop. Multiple epochs are required to reduce the loss to a minimal. Term Desc Iteration one forward/backward pass Batch Size a subsample of the training data, in one forward/backward pass (1 iteration) Epoch one forward/backward pass for ALL training samples (many iterations) Batch Normalization While a good activation function like ReLu or its variants can reduce the vanishing gradient problems, it might still return during later training. In 2015, a technique called Batch Normalization avoid this by normalizing and shifting the batch inputs to the mean. Other benefits are as listed. Faster convergence Decrease initial weights importance Robust to hyperparameters Requires less data for generalization Training will be slower as each epoch takes more than to compute the normalization. However, less epochs are usually required to reach convergence. However, the results will not be good if the batch size is very small, since the mean and variance will not be representative of the dataset. Other effects are listed in this article . Dropout Dropout is one of the most popular regularization techniques for DNN. Proposed in 2012, it is a simple algothrim; for every iteration, it randomly selects neurons to be ignored during training, and thus prevents overfitting . Dropout. From the book, Hands-on Machine Learning with Scikit-Learn, and TensorFlow The hyperparameter dropout rate usually set to 0.1-0.5, and is placed before a NN layer. Some tricks on adjusting the rate includes: Increase the dropout rate if the model is overfitting, and vice versa It might help to increase the dropout rate for large layers and reduce the smaller ones too. Many SOTA architectures only uses dropout dropout after the last hidden layer, so it might be worth a try if full dropout is too strong","title":"Basics"},{"location":"ann/#basics-of-neural-network","text":"A perceptron is the simplest unit of a neural network. The input neurons first weights its inputs and then sums them up with a constant, called bias . An activation function is then applied, which produces the output for the neuron. x1*w1 + x2*w2 + x3*w3 + bias = y A deep neural network simply represents a neural network with many hidden layers between the input and output layers. The architecture of the hidden layers can be very complex, like CNN or LSTM.","title":"Basics of Neural Network"},{"location":"ann/#activation-function","text":"An activation function tells a perception what outcome it should be. The function differs for input/hidden layers with output layers. For input/hidden layers, ReLu (Rectified Linear units) is very popular compared to the now mostly obsolete sigmoid & tanh functions because it avoids the vanishing gradient problem and has faster convergence. However, it is susceptible to dead neurons. So variants like Leaky ReLu, MaxOut and other functions are created to address this. For the output layer, it depends on the type of learning we are trying to train. Output Type Function Binary Classification Sigmoid Multi-Class Classification Softmax Regression Linear","title":"Activation Function"},{"location":"ann/#backpropagation","text":"Backpropagation is a short form for \"backward propagation of errors.\" Training a model is about minimising the loss, and to do that, fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration. This is done by working backwards from the last layer to the first input layer. There are various essential terms that needs to be defined in an NN during training.","title":"Backpropagation"},{"location":"ann/#optimizer","text":"Optimizers are learning algothrims used to change the weights & biases of the neural network optimially to obtain the minimal loss. Gradient descent is the most classic of them, while the most widely used optimizer now is Adam (Adaptive Moment Estimation). More in this article . Optimizer Desc Gradient Descent Most basic & classic Adam Most popular & the current best. Adaptive learning using EWMA on 1st & 2nd moments Rmsprop 2nd most popular. EWMA on squared gradient adagrad Adagrad Able to train for sparse data. Adaptive learning using squared gradient","title":"Optimizer"},{"location":"ann/#loss-function","text":"The loss (or cost) function which we define as the approximation for data loss. It is thus important to choose one that best represents the type of data and learning. Below are some examples. Type Loss Function Binary Classification binary_crossentropy Multi-class Classification categorical_crossentropy Regression mse","title":"Loss Function"},{"location":"ann/#learning-rate","text":"Learning Rate (lr), or step size is the most important parameter to adjust when using an optimizer. Too large a lr can cause the model to be unable to find the minimal loss, whereas a lr that is too small can cause the training process to take too long. Using a large vs small learning rate to reach minimum loss To check if you are using a good learning rate, we can plot the loss over epoch. It should ideally drop gradually to a consistent rate over time. Loss over epoch for varying learning rates. Source","title":"Learning Rate"},{"location":"ann/#batch-size-epoch","text":"Because of memory limitations, we cannot feed the entire training data to the network at one go, but divide them into batches. Given a total training sample size of 400, and a batch size of 4, we will require 400/4 = 100 iterations to complete one epoch which is the completion of training all sample sizes in a loop. Multiple epochs are required to reduce the loss to a minimal. Term Desc Iteration one forward/backward pass Batch Size a subsample of the training data, in one forward/backward pass (1 iteration) Epoch one forward/backward pass for ALL training samples (many iterations)","title":"Batch Size &amp; Epoch"},{"location":"ann/#batch-normalization","text":"While a good activation function like ReLu or its variants can reduce the vanishing gradient problems, it might still return during later training. In 2015, a technique called Batch Normalization avoid this by normalizing and shifting the batch inputs to the mean. Other benefits are as listed. Faster convergence Decrease initial weights importance Robust to hyperparameters Requires less data for generalization Training will be slower as each epoch takes more than to compute the normalization. However, less epochs are usually required to reach convergence. However, the results will not be good if the batch size is very small, since the mean and variance will not be representative of the dataset. Other effects are listed in this article .","title":"Batch Normalization"},{"location":"ann/#dropout","text":"Dropout is one of the most popular regularization techniques for DNN. Proposed in 2012, it is a simple algothrim; for every iteration, it randomly selects neurons to be ignored during training, and thus prevents overfitting . Dropout. From the book, Hands-on Machine Learning with Scikit-Learn, and TensorFlow The hyperparameter dropout rate usually set to 0.1-0.5, and is placed before a NN layer. Some tricks on adjusting the rate includes: Increase the dropout rate if the model is overfitting, and vice versa It might help to increase the dropout rate for large layers and reduce the smaller ones too. Many SOTA architectures only uses dropout dropout after the last hidden layer, so it might be worth a try if full dropout is too strong","title":"Dropout"},{"location":"cnn/","text":"CNN Convolutional Neural Networks (CNN) is a neural network architecture used mainly for image classification or object detection. A basic CNN architecture. Source Feature Selection Convolution The convolution layer extracts features from the image using kernels or filters. A kernel is a matrix of 2D numbers. The process of convolution is done by shifting the kernel over the image by strides (no. of pixel per shift), and computing the new value ( see this great explanation). A strike of more than 1 reduces the dimensions of the image. Different kernels can accentuate various features, like edge detection, sharpening or blurring. The convoluted layer is then passed to a ReLu activation function. The final output is called a feature map . In reality, a convulational layer has mutiple filters, and it outputs one feature map per filter. Full basic CNN architecture. Source Pooling The pooling layer shrinks the image to reduce the computational load, memory usage, and number of parameters, while retaining the important information. There are different spatial pooling techniques like Max Pooling (most common), Average Pooling or Sum Pooling. Classification The image matrix is flatten to a vector and feed to a normal linear fully connected network, and with the final layer ending with a softmax activation function. Datasets There are various large annotated image datasets that allows model training & benchmarking for image classification, detection, and segmentation. Name By Segmentation ImageNet Li Fei-Fei, Stanford No COCO Microsoft Yes","title":"CNN"},{"location":"cnn/#cnn","text":"Convolutional Neural Networks (CNN) is a neural network architecture used mainly for image classification or object detection. A basic CNN architecture. Source","title":"CNN"},{"location":"cnn/#feature-selection","text":"","title":"Feature Selection"},{"location":"cnn/#convolution","text":"The convolution layer extracts features from the image using kernels or filters. A kernel is a matrix of 2D numbers. The process of convolution is done by shifting the kernel over the image by strides (no. of pixel per shift), and computing the new value ( see this great explanation). A strike of more than 1 reduces the dimensions of the image. Different kernels can accentuate various features, like edge detection, sharpening or blurring. The convoluted layer is then passed to a ReLu activation function. The final output is called a feature map . In reality, a convulational layer has mutiple filters, and it outputs one feature map per filter. Full basic CNN architecture. Source","title":"Convolution"},{"location":"cnn/#pooling","text":"The pooling layer shrinks the image to reduce the computational load, memory usage, and number of parameters, while retaining the important information. There are different spatial pooling techniques like Max Pooling (most common), Average Pooling or Sum Pooling.","title":"Pooling"},{"location":"cnn/#classification","text":"The image matrix is flatten to a vector and feed to a normal linear fully connected network, and with the final layer ending with a softmax activation function.","title":"Classification"},{"location":"cnn/#datasets","text":"There are various large annotated image datasets that allows model training & benchmarking for image classification, detection, and segmentation. Name By Segmentation ImageNet Li Fei-Fei, Stanford No COCO Microsoft Yes","title":"Datasets"},{"location":"keras/","text":"Keras Keras is a neural network library, popular for its high level API, thus being extremely user friendly. It was developed by Fran\u00e7ois Chollet, who later joined Google. Keras was then merged with Tensorflow's library.","title":"Keras"},{"location":"keras/#keras","text":"Keras is a neural network library, popular for its high level API, thus being extremely user friendly. It was developed by Fran\u00e7ois Chollet, who later joined Google. Keras was then merged with Tensorflow's library.","title":"Keras"},{"location":"pytorch/","text":"Pytorch Pytorch is likely the most popular neural network framework given its ease of use compared to Tensorflow, and having more options then the high level framework Keras. It is developed by Facebook's AI Research lab. Tensors Tensors are used in place of numpy in Pytorch. This allows faster processing using GPUs. If as use torch.as_tensor or torch.tensor , it will infer the datatype from the original array and assign it as such. # convert array/list to pytorch tensor, retains a link to the array x = torch . as_tensor ( arr ) # convert array to tensor, no linkage, just a copy x = torch . tensor ( arr ) # check datatype x . dtype ... torch . int32 If we want to convert the tensor to specific datatypes, we can refer to the table below. Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor GPU Pytorch is able to use GPU to accelerate its processing speed. We can set cuda by writing an if-else clause. Sometimes, just adding cuda will not work, but we have to specify the device id, i.e. cuda:0 import torch print ( 'PyTorch Version:' , torch . __version__ ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) if device . type == 'cuda' : print ( 'Number of GPUs:' , torch . cuda . device_count ()) print ( 'Device properties:' , torch . cuda . get_device_properties ( 0 )) print ( 'Device ID:' , torch . cuda . current_device ()) print ( 'Device Name:' , torch . cuda . get_device_name ( 0 )) Number of GPUs: 1 Device properties: _CudaDeviceProperties ( name = 'Quadro P1000' , major = 6 , minor = 1 , total_memory = 4040MB, multi_processor_count = 4 ) Device ID: 0 Device Name: Quadro P1000 We can set the model to run in GPU, ideally by placing the device variable model.to(device) . # check if using gpu next ( model . parameters ()) . is_cuda # use gpu model . cuda () # or model . to ( device ) We can do the same for the tensors, to specify them to use the GPU. a = torch . FloatTensor ([ 1.0 , 2.0 ]) # check if using gpu a . device # use gpu a . cuda () # or a . to ( device ) Data Loader Here's a more specify example on a train-test split dataset. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) . to ( device ) X_test = torch . FloatTensor ( X_test ) . to ( device ) y_train = torch . LongTensor ( y_train ) . to ( device ) y_test = torch . LongTensor ( y_test ) . to ( device ) An easier way is to use TensorDataset & DataLoader . from torch.utils.data import TensorDataset , DataLoader iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) Pytorch also have another dataset libraries, including torchvision , torchtext , torchaudio . Modelling Model Class To build the model architecture, we need to assign it within a class, with the __init__ with the layers, and forward with the activation functions. We then initiate the model, and define the loss function & optimizer. import torch import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () # initiate nn.Module self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate model class torch . manual_seed ( 32 ) # so that initial weight & bias are same model = Model () # define loss & optimizer criterion = nn . CrossEntropyLoss () # model parameters are the layers' parameters of the NN optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 ) Train To train the model, we need to iterate the number of epochs, do a feed forward pass, then followed by a back-propagation. In Pytorch, we need to set the gradients to zero for each epoch optimizer.zero_grad() as it accumulates the gradients on each pass. epochs = 100 losses = [] for i in range ( epochs ): # feed forward y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) # backpropagation optimizer . zero_grad () loss . backward () optimizer . step () After training, using the validation set, we evaluate the model to see if there is good generalization. with torch . no_grad (): # not to update gradients y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( loss ) Save, Load & Predict We can either save the model only with the learnt paramters, using state_dict , or save both the learnt parameters & model class. Pytorch convention is to save the models with either .pt or .pth format. # save learnt parameters (biases & weights) only, but not model class torch . save ( model . state_dict (), 'best_model.pt' ) # load model = Model () model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) # save parameters & model class torch . save ( model . state_dict (), 'best_model.pt' ) # load model = torch . load ( 'best_model.pt' ) To predict on new data, we need to first switch to evaluate mode mode.eval() so that dropout and batch normalization layers are turned off. Then we will again, use torch.no_grad() and pass in the new data. model . eval () with torch . no_grad (): print ( model ( new_data )) # model output print ( model ( new_data ) . argmax ()) # max output Inference Optimization This Pytorch guide provides several tips to optimize trained models' inference time.","title":"Pytorch"},{"location":"pytorch/#pytorch","text":"Pytorch is likely the most popular neural network framework given its ease of use compared to Tensorflow, and having more options then the high level framework Keras. It is developed by Facebook's AI Research lab.","title":"Pytorch"},{"location":"pytorch/#tensors","text":"Tensors are used in place of numpy in Pytorch. This allows faster processing using GPUs. If as use torch.as_tensor or torch.tensor , it will infer the datatype from the original array and assign it as such. # convert array/list to pytorch tensor, retains a link to the array x = torch . as_tensor ( arr ) # convert array to tensor, no linkage, just a copy x = torch . tensor ( arr ) # check datatype x . dtype ... torch . int32 If we want to convert the tensor to specific datatypes, we can refer to the table below. Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor","title":"Tensors"},{"location":"pytorch/#gpu","text":"Pytorch is able to use GPU to accelerate its processing speed. We can set cuda by writing an if-else clause. Sometimes, just adding cuda will not work, but we have to specify the device id, i.e. cuda:0 import torch print ( 'PyTorch Version:' , torch . __version__ ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) if device . type == 'cuda' : print ( 'Number of GPUs:' , torch . cuda . device_count ()) print ( 'Device properties:' , torch . cuda . get_device_properties ( 0 )) print ( 'Device ID:' , torch . cuda . current_device ()) print ( 'Device Name:' , torch . cuda . get_device_name ( 0 )) Number of GPUs: 1 Device properties: _CudaDeviceProperties ( name = 'Quadro P1000' , major = 6 , minor = 1 , total_memory = 4040MB, multi_processor_count = 4 ) Device ID: 0 Device Name: Quadro P1000 We can set the model to run in GPU, ideally by placing the device variable model.to(device) . # check if using gpu next ( model . parameters ()) . is_cuda # use gpu model . cuda () # or model . to ( device ) We can do the same for the tensors, to specify them to use the GPU. a = torch . FloatTensor ([ 1.0 , 2.0 ]) # check if using gpu a . device # use gpu a . cuda () # or a . to ( device )","title":"GPU"},{"location":"pytorch/#data-loader","text":"Here's a more specify example on a train-test split dataset. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) . to ( device ) X_test = torch . FloatTensor ( X_test ) . to ( device ) y_train = torch . LongTensor ( y_train ) . to ( device ) y_test = torch . LongTensor ( y_test ) . to ( device ) An easier way is to use TensorDataset & DataLoader . from torch.utils.data import TensorDataset , DataLoader iris = TensorDataset ( torch . FloatTensor ( data ), torch . LongTensor ( labels )) iris_loader = DataLoader ( iris , batch_size = 105 , shuffle = True ) Pytorch also have another dataset libraries, including torchvision , torchtext , torchaudio .","title":"Data Loader"},{"location":"pytorch/#modelling","text":"","title":"Modelling"},{"location":"pytorch/#model-class","text":"To build the model architecture, we need to assign it within a class, with the __init__ with the layers, and forward with the activation functions. We then initiate the model, and define the loss function & optimizer. import torch import torch.nn as nn import torch.nn.functional as F class Model ( nn . Module ): def __init__ ( self , in_features = 4 , h1 = 8 , h2 = 9 , out_features = 3 ): super () . __init__ () # initiate nn.Module self . fc1 = nn . Linear ( in_features , h1 ) # input layer self . fc2 = nn . Linear ( h1 , h2 ) # hidden layer self . out = nn . Linear ( h2 , out_features ) # output layer def forward ( self , x ): x = F . relu ( self . fc1 ( x )) x = F . relu ( self . fc2 ( x )) x = self . out ( x ) return x # Instantiate model class torch . manual_seed ( 32 ) # so that initial weight & bias are same model = Model () # define loss & optimizer criterion = nn . CrossEntropyLoss () # model parameters are the layers' parameters of the NN optimizer = torch . optim . Adam ( model . parameters (), lr = 0.01 )","title":"Model Class"},{"location":"pytorch/#train","text":"To train the model, we need to iterate the number of epochs, do a feed forward pass, then followed by a back-propagation. In Pytorch, we need to set the gradients to zero for each epoch optimizer.zero_grad() as it accumulates the gradients on each pass. epochs = 100 losses = [] for i in range ( epochs ): # feed forward y_pred = model . forward ( X_train ) loss = criterion ( y_pred , y_train ) losses . append ( loss ) print ( f 'epoch: { i : 2 } loss: { loss . item () : 10.8f } ' ) # backpropagation optimizer . zero_grad () loss . backward () optimizer . step () After training, using the validation set, we evaluate the model to see if there is good generalization. with torch . no_grad (): # not to update gradients y_val = model . forward ( X_test ) loss = criterion ( y_val , y_test ) print ( loss )","title":"Train"},{"location":"pytorch/#save-load-predict","text":"We can either save the model only with the learnt paramters, using state_dict , or save both the learnt parameters & model class. Pytorch convention is to save the models with either .pt or .pth format. # save learnt parameters (biases & weights) only, but not model class torch . save ( model . state_dict (), 'best_model.pt' ) # load model = Model () model . load_state_dict ( torch . load ( 'IrisDatasetModel.pt' )) # save parameters & model class torch . save ( model . state_dict (), 'best_model.pt' ) # load model = torch . load ( 'best_model.pt' ) To predict on new data, we need to first switch to evaluate mode mode.eval() so that dropout and batch normalization layers are turned off. Then we will again, use torch.no_grad() and pass in the new data. model . eval () with torch . no_grad (): print ( model ( new_data )) # model output print ( model ( new_data ) . argmax ()) # max output","title":"Save, Load &amp; Predict"},{"location":"pytorch/#inference-optimization","text":"This Pytorch guide provides several tips to optimize trained models' inference time.","title":"Inference Optimization"},{"location":"tensorboard/","text":"","title":"Tensorboard"},{"location":"tensorflow/","text":"Tensorflow GPU import tensorflow as tf print ( 'TensorFlow Version:' , tf . __version__ ) device = tf . test . gpu_device_name () or 'CPU' print ( 'Using device:' , device )","title":"Tensorflow"},{"location":"tensorflow/#tensorflow","text":"","title":"Tensorflow"},{"location":"tensorflow/#gpu","text":"import tensorflow as tf print ( 'TensorFlow Version:' , tf . __version__ ) device = tf . test . gpu_device_name () or 'CPU' print ( 'Using device:' , device )","title":"GPU"},{"location":"transferlearning/","text":"","title":"Transferlearning"}]}