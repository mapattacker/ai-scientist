{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"So You Wana Be an AI Scientist? The evolution of people having the ability to intepret data has evolved over the years. Statistician, Data Analyst, Data Scientist, and now a new kid of the block, AI scientist is borne. They are more or less related in various ways, but the latter is predominantly involved in predictive learning solutions using Neural Networks (NN). This page will deal with the basics of NN, including the various frameworks, transfer-learning, and other tools that will allow one to kick start his/her career. Don't worry, it is not as hard once you get a grasp of it! x1*w1 + x2*w2 + x3*w3 + bias = y Do check out my accompanying website on AI Engineering that provides essential guides on engineering aspects of machine learning.","title":"Introduction"},{"location":"#so-you-wana-be-an-ai-scientist","text":"The evolution of people having the ability to intepret data has evolved over the years. Statistician, Data Analyst, Data Scientist, and now a new kid of the block, AI scientist is borne. They are more or less related in various ways, but the latter is predominantly involved in predictive learning solutions using Neural Networks (NN). This page will deal with the basics of NN, including the various frameworks, transfer-learning, and other tools that will allow one to kick start his/her career. Don't worry, it is not as hard once you get a grasp of it! x1*w1 + x2*w2 + x3*w3 + bias = y Do check out my accompanying website on AI Engineering that provides essential guides on engineering aspects of machine learning.","title":"So You Wana Be an AI Scientist?"},{"location":"ann/","text":"Basics of Neural Network A perceptron is the simplest unit of a neural network. The input neurons first weights its inputs and then sums them up with a bias. An activation function is then applied, which produces the output for the neuron. x1*w1 + x2*w2 + x3*w3 + bias = y A deep neural network simply represents a neural network with many hidden layers between the input and output layers. The architecture of the hidden layers can be very complex, like CNN or LSTM. Activation Function An activation function tells a perception what outcome it should be. The function differs for input/hidden layers with output layers. For input/hidden layers, ReLu (Rectified Linear units) is very popular compared to the now mostly obsolete sigmoid & tanh functions because it avoids the vanishing gradient problem and has faster convergence. However, it is susceptible to dead neurons. So variants like Leaky ReLu, MaxOut and other functions are created to address this. For the output layer, it depends on the type of learning we are trying to train. Output Type Function Binary Classification Sigmoid Multi-Class Classification Softmax Regression Linear Backpropagation Training a model is about minimising the loss, and to do that, we want to move in the negative direction of the derivative. Back-propagation is the process of calculating the derivatives. This is done by working backwards from the last layer to the first input layer. There are various essential terms that needs to be defined in an ANN during training. Term Desc Optimizers learning algothrims used to change the weights of the neural network optimially to obtain the minimal loss. Gradient descent is the most classic of them, while the most widely used optimizer now is Adam (Adaptive Moment Estimation). More in this article Loss Function function which we define as the approximation for data loss. It is thus important to choose one that best represents the type of data and learning Learning Rate (lr) is the most important parameter to adjust when using an optimizer. Too large a lr can cause the model to be unable to find the minimal loss, whereas a lr that is too small can cause the training process to take too long Using a large vs small learning rate Batch Size & Epoch Because of memory limitations, we cannot feed the entire training data to the network at one go, but divide them into batches. Given a total training sample size of 400, and a batch size of 4, we will require 400/4 = 100 iterations to complete one epoch which is the completion of training all sample sizes in a loop. Multiple epochs are required to reduce the loss to a minimal. Term Desc Iteration one forward/backward pass Batch Size a subsample of the training data, in one forward/backward pass (1 iteration) Epoch one forward/backward pass for ALL training samples (many iterations)","title":"Basics of Neural Networks"},{"location":"ann/#basics-of-neural-network","text":"A perceptron is the simplest unit of a neural network. The input neurons first weights its inputs and then sums them up with a bias. An activation function is then applied, which produces the output for the neuron. x1*w1 + x2*w2 + x3*w3 + bias = y A deep neural network simply represents a neural network with many hidden layers between the input and output layers. The architecture of the hidden layers can be very complex, like CNN or LSTM.","title":"Basics of Neural Network"},{"location":"ann/#activation-function","text":"An activation function tells a perception what outcome it should be. The function differs for input/hidden layers with output layers. For input/hidden layers, ReLu (Rectified Linear units) is very popular compared to the now mostly obsolete sigmoid & tanh functions because it avoids the vanishing gradient problem and has faster convergence. However, it is susceptible to dead neurons. So variants like Leaky ReLu, MaxOut and other functions are created to address this. For the output layer, it depends on the type of learning we are trying to train. Output Type Function Binary Classification Sigmoid Multi-Class Classification Softmax Regression Linear","title":"Activation Function"},{"location":"ann/#backpropagation","text":"Training a model is about minimising the loss, and to do that, we want to move in the negative direction of the derivative. Back-propagation is the process of calculating the derivatives. This is done by working backwards from the last layer to the first input layer. There are various essential terms that needs to be defined in an ANN during training. Term Desc Optimizers learning algothrims used to change the weights of the neural network optimially to obtain the minimal loss. Gradient descent is the most classic of them, while the most widely used optimizer now is Adam (Adaptive Moment Estimation). More in this article Loss Function function which we define as the approximation for data loss. It is thus important to choose one that best represents the type of data and learning Learning Rate (lr) is the most important parameter to adjust when using an optimizer. Too large a lr can cause the model to be unable to find the minimal loss, whereas a lr that is too small can cause the training process to take too long Using a large vs small learning rate","title":"Backpropagation"},{"location":"ann/#batch-size-epoch","text":"Because of memory limitations, we cannot feed the entire training data to the network at one go, but divide them into batches. Given a total training sample size of 400, and a batch size of 4, we will require 400/4 = 100 iterations to complete one epoch which is the completion of training all sample sizes in a loop. Multiple epochs are required to reduce the loss to a minimal. Term Desc Iteration one forward/backward pass Batch Size a subsample of the training data, in one forward/backward pass (1 iteration) Epoch one forward/backward pass for ALL training samples (many iterations)","title":"Batch Size &amp; Epoch"},{"location":"keras/","text":"Keras Keras is a neural network library, popular for its high level API, thus being extremely user friendly. It was developed by Fran\u00e7ois Chollet, who later joined Google. Keras was then merged with Tensorflow's library.","title":"Keras"},{"location":"keras/#keras","text":"Keras is a neural network library, popular for its high level API, thus being extremely user friendly. It was developed by Fran\u00e7ois Chollet, who later joined Google. Keras was then merged with Tensorflow's library.","title":"Keras"},{"location":"pytorch/","text":"Pytorch Pytorch is likely the most popular neural network framework given its ease of use compared to Tensorflow, and having more options then the high level framework Keras. It is developed by Facebook's AI Research lab. Tensors Tensors are used in place of numpy in Pytorch. This allows faster processing using GPUs. If as use torch.as_tensor or torch.tensor , it will infer the datatype from the original array and assign it as such. # convert array/list to pytorch tensor, retains a link to the array x = torch . as_tensor ( arr ) # convert array to tensor, no linkage, just a copy x = torch . tensor ( arr ) # check datatype x . dtype ... torch . int32 If we want to convert the tensor to specific datatypes, we can refer to the table below. Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor GPU Pytorch is able to use GPU to accelerate its processing speed. We can set cuda by writing an if-else clause. Sometimes, just adding cuda will not work, but we have to specify the device id, i.e. cuda:0 import torch print ( 'PyTorch Version:' , torch . __version__ ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) if device . type == 'cuda' : print ( 'Number of GPUs:' , torch . cuda . device_count ()) print ( 'Device properties:' , torch . cuda . get_device_properties ( 0 )) print ( 'Device ID:' , torch . cuda . current_device ()) print ( 'Device Name:' , torch . cuda . get_device_name ( 0 )) Number of GPUs: 1 Device properties: _CudaDeviceProperties ( name = 'Quadro P1000' , major = 6 , minor = 1 , total_memory = 4040MB, multi_processor_count = 4 ) Device ID: 0 Device Name: Quadro P1000 We can set the model to run in GPU, ideally by placing the device variable model.to(device) . # check if using gpu next ( model . parameters ()) . is_cuda # use gpu model . cuda () # or model . to ( device ) We can do the same for the tensors, to specify them to use the GPU. a = torch . FloatTensor ([ 1.0 , 2.0 ]) # check if using gpu a . device # use gpu a . cuda () # or a . to ( device ) Here's a more specify example on a train-test split dataset. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) . to ( device ) X_test = torch . FloatTensor ( X_test ) . to ( device ) y_train = torch . LongTensor ( y_train ) . to ( device ) y_test = torch . LongTensor ( y_test ) . to ( device )","title":"Pytorch"},{"location":"pytorch/#pytorch","text":"Pytorch is likely the most popular neural network framework given its ease of use compared to Tensorflow, and having more options then the high level framework Keras. It is developed by Facebook's AI Research lab.","title":"Pytorch"},{"location":"pytorch/#tensors","text":"Tensors are used in place of numpy in Pytorch. This allows faster processing using GPUs. If as use torch.as_tensor or torch.tensor , it will infer the datatype from the original array and assign it as such. # convert array/list to pytorch tensor, retains a link to the array x = torch . as_tensor ( arr ) # convert array to tensor, no linkage, just a copy x = torch . tensor ( arr ) # check datatype x . dtype ... torch . int32 If we want to convert the tensor to specific datatypes, we can refer to the table below. Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor","title":"Tensors"},{"location":"pytorch/#gpu","text":"Pytorch is able to use GPU to accelerate its processing speed. We can set cuda by writing an if-else clause. Sometimes, just adding cuda will not work, but we have to specify the device id, i.e. cuda:0 import torch print ( 'PyTorch Version:' , torch . __version__ ) device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) if device . type == 'cuda' : print ( 'Number of GPUs:' , torch . cuda . device_count ()) print ( 'Device properties:' , torch . cuda . get_device_properties ( 0 )) print ( 'Device ID:' , torch . cuda . current_device ()) print ( 'Device Name:' , torch . cuda . get_device_name ( 0 )) Number of GPUs: 1 Device properties: _CudaDeviceProperties ( name = 'Quadro P1000' , major = 6 , minor = 1 , total_memory = 4040MB, multi_processor_count = 4 ) Device ID: 0 Device Name: Quadro P1000 We can set the model to run in GPU, ideally by placing the device variable model.to(device) . # check if using gpu next ( model . parameters ()) . is_cuda # use gpu model . cuda () # or model . to ( device ) We can do the same for the tensors, to specify them to use the GPU. a = torch . FloatTensor ([ 1.0 , 2.0 ]) # check if using gpu a . device # use gpu a . cuda () # or a . to ( device ) Here's a more specify example on a train-test split dataset. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 33 ) X_train = torch . FloatTensor ( X_train ) . to ( device ) X_test = torch . FloatTensor ( X_test ) . to ( device ) y_train = torch . LongTensor ( y_train ) . to ( device ) y_test = torch . LongTensor ( y_test ) . to ( device )","title":"GPU"},{"location":"tensorboard/","text":"","title":"Tensorboard"},{"location":"tensorflow/","text":"Tensorflow GPU import tensorflow as tf print ( 'TensorFlow Version:' , tf . __version__ ) device = tf . test . gpu_device_name () or 'CPU' print ( 'Using device:' , device )","title":"Tensorflow"},{"location":"tensorflow/#tensorflow","text":"","title":"Tensorflow"},{"location":"tensorflow/#gpu","text":"import tensorflow as tf print ( 'TensorFlow Version:' , tf . __version__ ) device = tf . test . gpu_device_name () or 'CPU' print ( 'Using device:' , device )","title":"GPU"},{"location":"transferlearning/","text":"","title":"Transferlearning"}]}